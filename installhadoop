Prerequisites:(refer to http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/)
- create a new usergroup hadoop and new user hduser
- this user must have rights to sudo
- to give the sudo rights to the user:
	- add hduser ALL=(ALL) ALL  to /etc/sysctl.conf file(refer to this http://askubuntu.com/questions/124166/how-do-i-add-myself-into-the-sudoers-group)
		- to be able to change this file one must be root
			- you should know the root password
			- in case you don't know how to change it refer to : http://www.wikihow.com/Change-the-Root-Password-in-Linux

Refer to this documentation:
http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/

install java:  https://www.digitalocean.com/community/tutorials/how-to-install-java-with-apt-get-on-ubuntu-16-04
Installing the Default JRE/JDK
sudo apt-get update
sudo apt-get install default-jre
sudo apt-get install default-jdk

Installing the Oracle JDK
sudo add-apt-repository ppa:webupd8team/java
sudo apt-get update

Oracle JDK 8
sudo apt-get install oracle-java8-installer

install hadoop:
sudo wget http://apache.mirror.gtcomm.net/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz
sudo tar -xzvf hadoop-2.7.2.tar.gz
sudo mv hadoop-2.7.2 hadoop

Setup Environment Variable
cd
sudo vi ~/.bashrc
export HADOOP_CONF=/home/ubuntu/hadoop/conf
export HADOOP_PREFIX=/home/ubuntu/hadoop
#Set JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-8-oracle/
# Add Hadoop bin/ directory to path
export PATH=$PATH:$HADOOP_PREFIX/bin

To check whether its been updated correctly or not, reload bash profile, use following commands

source ~/.bashrc
echo $HADOOP_PREFIX
echo $HADOOP_CONF

In the distribution, edit the file etc/hadoop/hadoop-env.sh to define some parameters as follows:
# set to the root of your Java installation
export JAVA_HOME=/usr/lib/jvm/java-8-oracle/

-configuration:
etc/hadoop/core-site.xml:
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>

-etc/hadoop/hdfs-site.xml:
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
</configuration>

- setup ssh on the machine:
http://cloudfront.blogspot.in/2012/07/how-to-setup-and-configure-ssh-on-ubuntu.html#.UcvbF0AW38t
sudo apt-get install ssh

- Setup passphraseless ssh
Now check that you can ssh to the localhost without a passphrase:
ssh localhost

If you cannot ssh to localhost without a passphrase, execute the following commands:
  $ ssh-keygen -t dsa -P '' -f ~/.ssh/id_dsa
  $ cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
  $ chmod 0600 ~/.ssh/authorized_keys


-Execution:  
Format the filesystem: bin/hdfs namenode -format

hadoop folder should have permissions : sudo chmod 777 hadoop

Start NameNode daemon and DataNode daemon: sbin/start-dfs.sh

Browse the web interface for the NameNode; by default it is available at: NameNode - http://localhost:50070/

Make the HDFS directories required to execute MapReduce jobs:
bin/hdfs dfs -mkdir /user
bin/hdfs dfs -mkdir /user/hduser

Copy the input files into the distributed filesystem: bin/hdfs dfs -put etc/hadoop input

-YARN on a Single Node
1. Configure parameters as follows:etc/hadoop/mapred-site.xml:
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

etc/hadoop/yarn-site.xml:
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
</configuration>

2. Start ResourceManager daemon and NodeManager daemon:
sbin/start-yarn.sh
3. Browse the web interface for the ResourceManager; by default it is available at:
http://localhost:8088/
4. When youâ€™re done, stop the daemons with:
sbin/stop-yarn.sh




